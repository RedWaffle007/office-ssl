{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dc3560c-c31e-4bd9-af0b-e24435426281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Added to sys.path: /home/syed-mohammed-bilal/projects/office-ssl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syed-mohammed-bilal/.local/share/mamba/envs/office_ssl/lib/python3.11/site-packages/lightning_fabric/__init__.py:29: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  __import__(\"pkg_resources\").declare_namespace(__name__)\n"
     ]
    }
   ],
   "source": [
    "#imports and paths\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root (one level up from 'notebooks') to Python path\n",
    "ROOT = Path(__file__).resolve().parents[1] if \"__file__\" in globals() else Path.cwd().parent\n",
    "sys.path.append(str(ROOT))\n",
    "\n",
    "print(\" Added to sys.path:\", ROOT)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.datasets import MultiLabelSceneDataset\n",
    "from src.finetune import MultiLabelFineTune\n",
    "from src.simclr import SimCLRModel\n",
    "\n",
    "BASE = Path.home() / \"projects/office-ssl\"\n",
    "LABELED = BASE / \"data/labeled\"\n",
    "\n",
    "IMG_DIR = LABELED / \"images\"\n",
    "ANNOT_CSV = LABELED / \"annotations.csv\"\n",
    "LABEL_MAP = LABELED / \"label_map.json\"\n",
    "CKPT_DIR = BASE / \"checkpoints\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d3f7757-95f0-4be3-a8c6-de934488c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633e2799-6eab-48a1-9a48-a0dc9f2a1fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets and Dataloaders\n",
    "train_ds = MultiLabelSceneDataset(IMG_DIR, ANNOT_CSV, LABEL_MAP)\n",
    "\n",
    "indices = list(range(len(train_ds)))\n",
    "train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "train_dl = DataLoader(Subset(train_ds, train_idx), batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_dl   = DataLoader(Subset(train_ds, val_idx), batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "print(f\"Train: {len(train_idx)} | Val: {len(val_idx)} | Classes: {len(json.load(open(LABEL_MAP)))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3607f8a-76d9-4be3-97c5-0bbb9abcdb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load simCLR backbone\n",
    "simclr_ckpt = sorted(CKPT_DIR.glob(\"simclr-epoch*.ckpt\"))[-1]\n",
    "simclr = SimCLRModel.load_from_checkpoint(str(simclr_ckpt), strict=False)\n",
    "\n",
    "backbone = simclr.backbone.to(device)\n",
    "backbone.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79fc60e-b414-4d86-b611-92ec042d7ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading fine tuned classifier\n",
    "from src.finetune import MultiLabelFineTune\n",
    "label_map = json.load(open(LABEL_MAP))\n",
    "num_classes = len(label_map)\n",
    "\n",
    "model = MultiLabelFineTune(\n",
    "    backbone=backbone,\n",
    "    num_classes=num_classes,\n",
    "    lr=1e-4,\n",
    "    freeze_backbone=False\n",
    ")\n",
    "\n",
    "# Load fine-tuned weights\n",
    "best_ckpt = sorted(CKPT_DIR.glob(\"finetune-*.ckpt\"))[-1]\n",
    "ft_state = torch.load(best_ckpt, map_location=device)[\"state_dict\"]\n",
    "\n",
    "clean_state = {}\n",
    "for k, v in ft_state.items():\n",
    "    if k.startswith(\"backbone.\") or k.startswith(\"classifier.\"):\n",
    "        clean_state[k] = v\n",
    "\n",
    "model.load_state_dict(clean_state, strict=False)\n",
    "model = model.to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ae229e-6502-44f3-af22-84998efd1417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ft_ckpt = torch.load(best_ckpt, map_location=device)\n",
    "# state = ft_ckpt[\"state_dict\"]\n",
    "\n",
    "# # remove prefix 'model.' or 'backbone.' if needed\n",
    "# new_state = {}\n",
    "# for k, v in state.items():\n",
    "#     if k.startswith(\"backbone.\"):\n",
    "#         new_state[k.replace(\"backbone.\", \"backbone.\")] = v\n",
    "#     elif k.startswith(\"classifier.\"):\n",
    "#         new_state[k] = v\n",
    "#     else:\n",
    "#         new_state[k] = v\n",
    "\n",
    "# model.load_state_dict(new_state, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc56cbb8-b5f7-42e9-9060-870931b70137",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(\"Backbone device:\", next(model.backbone.parameters()).device)\n",
    "print(\"Classifier device:\", next(model.classifier.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47e528c-9cdb-4994-a903-d7ff8ab6d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting avif files to jpg\n",
    "import subprocess\n",
    "\n",
    "# path to your labeled images\n",
    "img_dir = Path.home() / \"projects/office-ssl/data/labeled/images\"\n",
    "\n",
    "converted = 0\n",
    "for img_path in img_dir.glob(\"*.jpg\"):\n",
    "    result = subprocess.run([\"file\", str(img_path)], capture_output=True, text=True)\n",
    "    if \"AVIF\" in result.stdout:\n",
    "        out_path = img_path.with_name(img_path.stem + \"_fixed.jpg\")\n",
    "        print(f\"Converting {img_path.name} → {out_path.name}\")\n",
    "        subprocess.run([\"heif-convert\", str(img_path), str(out_path)], check=True)\n",
    "        out_path.rename(img_path)\n",
    "        converted += 1\n",
    "\n",
    "print(f\"Done! Converted {converted} AVIF images to JPEG.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdfae31-40e3-45bf-84c2-c53df337076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer + callbacks\n",
    "logger = TensorBoardLogger(\"logs\", name=\"finetune_multilabel\")\n",
    "ckpt_cb = ModelCheckpoint(\n",
    "    dirpath=str(CKPT_DIR),\n",
    "    filename=\"finetune-{epoch:02d}-{val_mAP_mean:.4f}\",\n",
    "    save_top_k=3,\n",
    "    monitor=\"val/mAP_mean\",\n",
    "    mode=\"max\",\n",
    "    auto_insert_metric_name=False\n",
    ")\n",
    "early_stop = EarlyStopping(monitor=\"val/mAP_mean\", mode=\"max\", patience=8)\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    devices=1,\n",
    "    max_epochs=50,\n",
    "    precision=16 if torch.cuda.is_available() else 32,\n",
    "    callbacks=[ckpt_cb, early_stop, lr_monitor],\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077eec40-1c33-4488-bcdc-76cc99f7bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load simCLR backbone\n",
    "# # latest pretrain checkpoint\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# label_map = json.load(open(LABEL_MAP))\n",
    "# num_classes = len(label_map)\n",
    "\n",
    "# best_ckpt = sorted((CKPT_DIR).glob(\"finetune-*.ckpt\"))[-1]\n",
    "# print(\"Loading checkpoint:\", best_ckpt)\n",
    "\n",
    "# # Load model from checkpoint\n",
    "# model = MultiLabelFineTune.load_from_checkpoint(\n",
    "#     str(best_ckpt),\n",
    "#     backbone=None,\n",
    "#     num_classes=num_classes\n",
    "# )\n",
    "\n",
    "# # --- THE IMPORTANT PART ---\n",
    "# # Move backbone AND classifier separately onto GPU\n",
    "# model.backbone = model.backbone.to(device)\n",
    "# model.classifier = model.classifier.to(device)\n",
    "# model = model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# print(\"Backbone device:\", next(model.backbone.parameters()).device)\n",
    "# print(\"Classifier device:\", next(model.classifier.parameters()).device)\n",
    "# print(\"Model device:\", next(model.parameters()).device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a31d902-850a-4a3a-abc2-a54be46504a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Always set a single variable and reuse it for consistency for same device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)\n",
    "\n",
    "# x, y = next(iter(train_dl))\n",
    "# x, y = x.to(device), y.to(device)\n",
    "# with torch.no_grad():\n",
    "#     loss = model.criterion(model(x), y)\n",
    "# print(f\"Initial loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f525bd-108f-49e3-86ac-5ff5942e9457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking is loss actually decreases\n",
    "batch = next(iter(train_dl))\n",
    "x, y = batch\n",
    "with torch.no_grad():\n",
    "    logits = model(x.to('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    loss = model.criterion(logits, y.to('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "print(\"Initial loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54aec414-d802-4c11-8d3d-3016c8135658",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "trainer.fit(model, train_dataloaders=train_dl, val_dataloaders=val_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7203f5f-9864-4130-a560-f5274591c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluating best model\n",
    "best_ckpt = ckpt_cb.best_model_path\n",
    "print(\"Best checkpoint:\", best_ckpt)\n",
    "\n",
    "finetuned = MultiLabelFineTune.load_from_checkpoint(best_ckpt, backbone=backbone, num_classes=num_classes)\n",
    "finetuned = finetuned.eval().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e913449c-ff58-4c57-9f61-b2147fecaefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score, f1_score, hamming_loss\n",
    "\n",
    "dl = DataLoader(train_ds, batch_size=32, shuffle=False, num_workers=4)\n",
    "all_preds, all_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in dl:\n",
    "        x = x.to(device)\n",
    "        preds = torch.sigmoid(finetuned(x)).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_targets.append(y.numpy())\n",
    "preds = np.vstack(all_preds)\n",
    "targets = np.vstack(all_targets)\n",
    "\n",
    "aps = [average_precision_score(targets[:,i], preds[:,i]) if targets[:,i].sum() else np.nan\n",
    "       for i in range(targets.shape[1])]\n",
    "print(\"Mean AP:\", np.nanmean(aps))\n",
    "print(\"Macro-F1:\", f1_score(targets, (preds>=0.5).astype(int), average='macro', zero_division=0))\n",
    "print(\"Hamming-loss:\", hamming_loss(targets, (preds>=0.5).astype(int)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aa7dbe-63a4-468f-858a-3317be97b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(finetuned.state_dict(), CKPT_DIR / \"finetune_best_state.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5070f34-3a72-468c-a42a-bc57ffc283fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no class is passing 0.5 confidence hence\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(preds.flatten(), bins=50)\n",
    "plt.title(\"Distribution of predicted probabilities\")\n",
    "plt.xlabel(\"p(class)\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82dbef1-99d6-4355-a8e9-b9e399a639c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confidence calibration\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "\n",
    "best_thresh = []\n",
    "for i in range(targets.shape[1]):\n",
    "    if targets[:,i].sum() == 0:\n",
    "        best_thresh.append(0.5)\n",
    "        continue\n",
    "    p, r, t = precision_recall_curve(targets[:,i], preds[:,i])\n",
    "    f1_scores = 2 * p * r / (p + r + 1e-12)\n",
    "    best_idx = np.nanargmax(f1_scores)\n",
    "    best_thresh.append(t[best_idx] if best_idx < len(t) else 0.5)\n",
    "\n",
    "print(\"Optimal thresholds per class:\")\n",
    "print(best_thresh)\n",
    "print(\"Mean threshold:\", np.mean(best_thresh))\n",
    "\n",
    "# recompute F1 with these thresholds\n",
    "pred_bin_opt = (preds >= np.array(best_thresh)).astype(int)\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 (optimal thresholds):\", f1_score(targets, pred_bin_opt, average='macro', zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5f2e50-5484-48e0-8325-a467bfd3de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class frquencies\n",
    "import pandas as pd\n",
    "annot = pd.read_csv(ANNOT_CSV)\n",
    "freqs = annot['labels'].str.split(';').explode().value_counts()\n",
    "print(freqs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e629a3a8-e568-447c-b6fd-fb3d32a2acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#per class avg bar chart\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "BASE = Path.home() / \"projects/office-ssl\"\n",
    "LABEL_MAP = BASE / \"data/labeled/label_map.json\"\n",
    "\n",
    "with open(LABEL_MAP, \"r\") as f:\n",
    "    label_map = json.load(f)\n",
    "\n",
    "idx_to_name = {v:k for k,v in label_map.items()}\n",
    "\n",
    "print(\"Loaded\", len(label_map), \"classes.\")\n",
    "\n",
    "names = list(label_map.keys())\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(np.arange(len(names)), aps)\n",
    "plt.xticks(np.arange(len(names)), names, rotation=90)\n",
    "plt.ylabel(\"Average Precision\")\n",
    "plt.title(\"Per-class AP after fine-tuning\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cb2aa5-53ad-486f-90a0-15c459021795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion-style heatmap\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "\n",
    "cms = multilabel_confusion_matrix(targets, (preds>=np.array(best_thresh)).astype(int))\n",
    "mAPs = [round(a,2) if not np.isnan(a) else 0 for a in aps]\n",
    "fig, axes = plt.subplots(4,5, figsize=(18,14))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    if i < len(names):\n",
    "        cm = cms[i]\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cbar=False, ax=ax)\n",
    "        ax.set_title(f\"{names[i]} (AP={mAPs[i]})\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99783527-e297-40c7-9cb6-d2775c3b88b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#t-SNE Visualization of embeddings\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "subset_idx = np.random.choice(len(train_ds), size=300, replace=False)\n",
    "\n",
    "embeddings = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in subset_idx:\n",
    "        img, label_vec = train_ds[idx]\n",
    "\n",
    "        # Move image to GPU\n",
    "        x = img.unsqueeze(0).to(device)\n",
    "\n",
    "        # 1) Extract backbone features\n",
    "        feat = model.backbone(x)\n",
    "\n",
    "        if feat.ndim == 4:\n",
    "            feat = torch.nn.functional.adaptive_avg_pool2d(feat, (1,1)).reshape(feat.size(0), -1)\n",
    "\n",
    "        embeddings.append(feat.cpu().numpy().squeeze())\n",
    "\n",
    "        # 2) Get classifier logits (also on GPU)\n",
    "        logits = model(x)\n",
    "        logits = logits.cpu().numpy().squeeze()\n",
    "\n",
    "        # Convert multi-label → single label for visualization\n",
    "        labels.append(np.argmax(logits))\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(\"Running t-SNE…\")\n",
    "tsne = TSNE(n_components=2, perplexity=30, learning_rate=200, n_iter=1500)\n",
    "emb_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(emb_2d[:,0], emb_2d[:,1], c=labels, cmap='tab20', s=10)\n",
    "plt.title(\"t-SNE visualization of fine-tuned embeddings\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d33121f-d82b-4b02-9c40-0387e31be6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (office_ssl)",
   "language": "python",
   "name": "office_ssl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
